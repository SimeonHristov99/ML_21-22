{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervised_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdj+uRndw6iVNQXJ6PaW+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SimeonHristov99/ML_21-22/blob/main/unsupervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised Learning"
      ],
      "metadata": {
        "id": "C8et0b2teTEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **unsupervised learning**, the dataset is a collection of **unlabeled examples** $\\{\\textbf{x}_i\\}_{i=0}^N$\n",
        "Again, **x** is a feature vector, and the goal of an unsupervised learning algorithm is\n",
        "to create a model that takes a feature vector **x** as input and either transforms it into\n",
        "another vector or into a value that can be used to solve a practical problem."
      ],
      "metadata": {
        "id": "0McIqZBTcNmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of problems\n",
        "\n",
        "- **clustering**: the model returns the id of the cluster for each feature vector in the dataset;\n",
        "- **dimensionality reduction**: the output of the model is a feature vector that has fewer features than the input **x**;\n",
        "- **outlier detection**: the output is a real number that indicates\n",
        "how **x** is different from a “typical” example in the dataset."
      ],
      "metadata": {
        "id": "QyCkO0z_GMat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "from sklearn.metrics.cluster import adjusted_rand_score"
      ],
      "metadata": {
        "id": "19PopgLelF-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIGSIZE = (12, 10)\n",
        "\n",
        "plt.rc('figure', figsize=FIGSIZE)\n",
        "sns.set_style('whitegrid')"
      ],
      "metadata": {
        "id": "3ElmzEZAlbIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "**Clustering** is a problem of learning to assign a label to examples by leveraging an unlabeled\n",
        "dataset. Because the dataset is completely unlabeled, deciding on whether the learned model\n",
        "is optimal is much more complicated than in supervised learning.\n",
        "\n",
        "> **Definition**: A cluster is a group of similar points."
      ],
      "metadata": {
        "id": "DQ_cEP8BgoVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means\n",
        "\n",
        "- Divides the dataset into `k` similar groups.\n",
        "- How does it work? First, you choose `k` — the number of\n",
        "clusters. Then you randomly put `k` feature vectors, called centroids, to the feature space.\n",
        "We then compute the distance from each example `x` to each centroid `c` using some metric,\n",
        "like the Euclidean distance. Then we assign the closest centroid to each example (like if we labeled each example with a centroid id as the label). For each centroid, we calculate the average feature vector of the examples labeled with it. These average feature vectors become the new locations of the centroids. We recompute the distance from each example to each centroid, modify the assignment and\n",
        "repeat the procedure until the assignments don’t change after the centroid locations were\n",
        "recomputed. The model is the list of assignments of centroids IDs to the examples.\n",
        "- Advantages:\n",
        "  - fast;\n",
        "  - scales well to large number of samples;\n",
        "  - has been used across a large range of application areas in many different fields.\n",
        "- Disadvantages:\n",
        "  - the initial position of centroids influence the final positions, so two runs of k-means can\n",
        "result in two different models;\n",
        "  - the value of `k`, the number of clusters, is a hyperparameter that can be tuned. There are no proven optimal techniques for selecting `k`. Most of them require an `educated guess` by looking at some\n",
        "metrics or by examining cluster assignments visually;\n",
        "  - cannot deal with non-linear cluster shapes."
      ],
      "metadata": {
        "id": "Ur1gf80LgmSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYrjtldbVCzm"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(random_state=1)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "rgTWGleClXwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1])"
      ],
      "metadata": {
        "id": "v4Ok90gGlVeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll try to get the following diagram but by using K-Means instead of the true labels `y`."
      ],
      "metadata": {
        "id": "TzLZ3FQGmYgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)"
      ],
      "metadata": {
        "id": "6J8Mk1X1lvA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: It may be obvious but the `.fit` method **does not** accept `y`."
      ],
      "metadata": {
        "id": "BaLxkzW2msOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trying out an example"
      ],
      "metadata": {
        "id": "4_5Rd1qUutBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=2)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "ZDVYZhommomy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: `y_pred` does not necessarily equal `y`. "
      ],
      "metadata": {
        "id": "aBSqFLxNnETz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "1lBeZ0JtnSAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)"
      ],
      "metadata": {
        "id": "_CL1O7TEmo5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring performance"
      ],
      "metadata": {
        "id": "49lCMCQQul-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we had the true labels**, we could measure the performance of our model by using the adjusted Rand index. It is a function that measures the similarity of the two assignments, ignoring permutations. Note that this is not accuracy - it is a measure of similarity.\n",
        "\n",
        "More on Rand index: https://www.youtube.com/watch?v=6rjTIwn0yWI, https://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-score"
      ],
      "metadata": {
        "id": "HzPxP-tmn7-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we didn't know the true labels**, we could use the built-in `.score` method. It returns the negative of the K-means objective. \n",
        "\n",
        "**K-Means Objective**\n",
        "\n",
        "The objective in the K-means is to reduce the sum of squares of the distances of points from their respective cluster centroids. It has other names like J-Squared error function, J-score or within-cluster sum of squares. This value tells how internally coherent the clusters are (the less the better).\n",
        "\n",
        "> **Note**: Although the result from `.score` might be low, this **does not guarantee** that the clusters are appropriate."
      ],
      "metadata": {
        "id": "CgEergtFwnEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adjusted_rand_score(y, y_pred)"
      ],
      "metadata": {
        "id": "8Q8YGqPRmpFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.score(X)"
      ],
      "metadata": {
        "id": "_h1D4kHbwpVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "9GlkwoAMmpT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don't know the number of clusters, K-Means is not that helpful."
      ],
      "metadata": {
        "id": "Z-4yupx3q73N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=2)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
        "\n",
        "print(adjusted_rand_score(y, y_pred))\n",
        "print(kmeans.score(X))"
      ],
      "metadata": {
        "id": "XercPxktmpgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=5)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
        "\n",
        "print(adjusted_rand_score(y, y_pred))\n",
        "print(kmeans.score(X))"
      ],
      "metadata": {
        "id": "k849VPLemp0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The shape matters"
      ],
      "metadata": {
        "id": "_OcQAGAWronU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(random_state=170, n_samples=600)\n",
        "\n",
        "random = np.random.RandomState(74)\n",
        "\n",
        "transformation = random.normal(size=(2, 2))\n",
        "\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1])"
      ],
      "metadata": {
        "id": "D5bJQWL6rEUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
        "ax = sns.scatterplot(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], palette='orange', s=500)"
      ],
      "metadata": {
        "id": "6N1ih4GPrmyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "y_pred = kmeans.predict(X)\n",
        "\n",
        "ax = sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred)\n",
        "ax = sns.scatterplot(x=kmeans.cluster_centers_[:, 0], y=kmeans.cluster_centers_[:, 1], palette='orange', s=500)"
      ],
      "metadata": {
        "id": "hun_1M_1rm56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agglomerative Clustering\n",
        "\n",
        "How does it work?\n",
        "\n",
        "1. Every point becomes a cluster.\n",
        "2. The most \"similar\" clusters get combined into one.\n",
        "3. Repeats step 2 until there are $k$ clusters.\n",
        "\n",
        "In Sklean the definition of similarity is controlled by the `affinity` and `linkage` [hyperparameters](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html).\n",
        "\n",
        "Disadvantages:\n",
        "  - requires `k`\n",
        "  - cannot deal with non-linear shapes\n",
        "  - does not have a `.predict` method"
      ],
      "metadata": {
        "id": "x8Mn-WGJydsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Alt](https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif)"
      ],
      "metadata": {
        "id": "WJjlMLCw0G3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(random_state=1)\n",
        "\n",
        "y_pred = AgglomerativeClustering(n_clusters=3).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7TBJ5Nfbrm-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "y_pred = AgglomerativeClustering(n_clusters=2).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ppo1t1ZMrnCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN\n",
        "\n",
        "While k-means and similar algorithms are **centroid-based**, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a **density-based**\n",
        "clustering algorithm. Instead of guessing how many clusters you need, by using DBSCAN,\n",
        "you define two hyperparameters: \u000f$ϵ$ and $n$.\n",
        "\n",
        "How does it work?\n",
        "\n",
        "- two main parameters `eps` and `min_samples`;\n",
        "- points get classified either as `core sample`, `boundary points`, or `noise`;\n",
        "  - core samples are points with `min_samples` or more neighbors that are at distance `eps` or less;\n",
        "  - boundary points are samples that are neighbors of a core sample in the cluster but are not themselves core samples (do not have `min_samples` neighbors);\n",
        "  - noise is everything else.\n",
        "\n",
        "Advantages:\n",
        "  - does not require a number of clusters\n",
        "  - can work with any shapes\n",
        "\n",
        "Disadvantages:\n",
        "  - slow\n",
        "  - has two hyperparameters and choosing good\n",
        "values for them (especially `eps`\u000f) could be challenging. Furthermore, having `eps`\u000f fixed, the clustering\n",
        "algorithm cannot effectively deal with clusters of varying density.\n",
        "\n",
        "More on DBSCAN: https://scikit-learn.org/stable/modules/clustering.html#dbscan, https://www.youtube.com/watch?v=RDZUdRSDOok"
      ],
      "metadata": {
        "id": "rioljXyd2LXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Alt](https://dashee87.github.io/images/DBSCAN_tutorial.gif)"
      ],
      "metadata": {
        "id": "lWEfW-sp81QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "y_pred = DBSCAN(eps=0.2).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l0pMNLqak5rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With an appropriate value for `eps`, DBSCAN can handle the strange three clusters we say earlier."
      ],
      "metadata": {
        "id": "7VwqUCvw_4PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(random_state=170, n_samples=600)\n",
        "\n",
        "random = np.random.RandomState(74)\n",
        "\n",
        "transformation = random.normal(size=(2, 2))\n",
        "\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.4)\n",
        "clusters = dbscan.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=clusters, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_tMrnXIn92bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HDBSCAN \n",
        "\n",
        "The clustering algorithm that keeps the advantages of DBSCAN while removing the need to decide on the value of `eps`\u000f. The algorithm is capable of building clusters of\n",
        "varying density.\n",
        "\n",
        "It has one important hyperparameter: `n` - the minimum number of examples to put in a cluster. This hyperparameter is relatively simple to choose by intuition.\n",
        "\n",
        "> **Note**: Because this algorithm is not implemented in sklearn, we have to install another package that provides it - https://pypi.org/project/hdbscan/.\n",
        "\n",
        "Advantage:\n",
        "\n",
        "- no hyperparameter tuning"
      ],
      "metadata": {
        "id": "Bn8Yr0tL7WvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan"
      ],
      "metadata": {
        "id": "fkcY6wfytuJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "X, _ = make_blobs(1000)\n",
        "\n",
        "y_pred = HDBSCAN(min_cluster_size=10).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJmkOLzgCwwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(random_state=170, n_samples=600)\n",
        "\n",
        "random = np.random.RandomState(74)\n",
        "\n",
        "transformation = random.normal(size=(2, 2))\n",
        "\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "y_pred = HDBSCAN(min_cluster_size=10).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qiMTieG5DZ94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "y_pred = HDBSCAN(min_cluster_size=10).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCt99l_ZEUWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "Algorithms for reducing the dimensionality of a dataset while preserving as much information as possible. The ultimate goal is to find the few `latent` (hidden) features that can be used to determine the outcome.\n",
        "\n",
        "Two ways to do dimensionality reduction:\n",
        "  - matrix factorization\n",
        "    - **principle component analysis (PCA)**\n",
        "    - linear autoencoder\n",
        "    - latent dirichlet allocation\n",
        "    - non-negative matrix factorization\n",
        "    - generalized low rank models\n",
        "    - Word2Vec\n",
        "    - GloVe\n",
        "  - neighbor graphs\n",
        "    - laplacian eigenmaps\n",
        "    - hessian eigenmaps\n",
        "    - local tangent space alignment\n",
        "    - JSE\n",
        "    - isomap\n",
        "    - **t-distributed stochastic neighbor embedding (t-SNE)**\n",
        "    - locally linear embedding\n",
        "    - **UMAP**"
      ],
      "metadata": {
        "id": "0EBdTBXZCAiR"
      }
    }
  ]
}